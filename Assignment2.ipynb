{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This assignment may be worked individually or in pairs. \n",
    "## Enter your name/names here:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#names here\n",
    "#Abhishek Dayal\n",
    "#Nathan Daniel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Assignment 2: Naive Bayes and KNN classifier\n",
    "\n",
    "In this assignment you'll implement the Naive Bayes and KNN classifiers to classify patients as either having or not having diabetic retinopathy. For this task we'll be using the same Diabetic Retinopathy data set which was used in the previous assignment on decision trees. The implementation details are up to you but, generally it is a good idea to divide your code up into helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Headers\n",
    "# You are welcome to add additional headers if you wish\n",
    "# EXCEPT for scikit-learn... You may NOT use scikit-learn for this assignment!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data from a CSV file. You may choose to store it any any format you wish, like a Pandas dataframe, or any other data structure you'd like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "    data = []\n",
    "#     your code goes here\n",
    "    data = pd.read_csv(filename, header=None)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Naive Bayes Classifier\n",
    "\n",
    "Naive Bayes (NB) classifier is a simple probabilistic classifier that is based on applying the Bayes' theorem and assumes a strong (naive) independence between features. The Diabetic Retinopathy data set contains both categorical and continuous features. Dealing with categorical features has been discussed in detail in class. Continuous attributes, on the other hand, are more interesting to handle. Most commonly, this is done by assuming normal probability distribution over the feature values or by binning the attribute values in a fixed number of bins. In this assignment you'll be implementing the binning approach. For each continuous attribute, you'll construct 3 equal sized bins. For example, feature 5 ranges from `[1 - 120]` the 3 bins that you'll construct will be `[1 - 40]`, `[41 - 80]`, `[81 - 120]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Implement a Naive Bayes classifier. Measure the accuracy of your classifier using 5-fold cross validation and display the confusion matrix. Also print the precision and recall for class label 1 (patients that have been diagnosed with the disease)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREP\n",
    "# bin continuous attributes (needs to be done with subset of df that is training data)\n",
    "def bin_df(df):\n",
    "    bins_dict = dict()\n",
    "    for c in range(2,18):\n",
    "        out, bins = pd.cut(df[c], 3, retbins=True, labels=[1,2,3])\n",
    "        bins_list = list(bins)\n",
    "        bins_list[0] = np.iinfo(np.int32).min\n",
    "        bins_list[-1] = np.iinfo(np.int32).max\n",
    "        bins_dict[c] = bins_list\n",
    "        \n",
    "        df[c] = out\n",
    "\n",
    "    df.head()\n",
    "    return (df, bins_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_test(df, bins_dict):\n",
    "    # bins_dict (K,V) = (feature #, bin threshold list)\n",
    "    binned_df = df.copy()\n",
    "    for c in range(2,18):\n",
    "        ii = pd.IntervalIndex.from_breaks(bins_dict[c])\n",
    "        column_binned = pd.cut(binned_df[c].to_list(), ii)\n",
    "        column_binned.categories = [1,2,3]\n",
    "        binned_df[c] = column_binned\n",
    "    \n",
    "#    binned_df.head()\n",
    "    return binned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_classify(test_record, training_df):\n",
    "    #CLASSIFY\n",
    "    # maybe use groupBy?\n",
    "    # calculate ~probability of 1\n",
    "        # P(record[0]| class=1) * ... * P(record[18]| class=1) * P(class=1)\n",
    "        \n",
    "    prior_1 = training_df[training_df[19] == 1].count()[0]/training_df.shape[0]\n",
    "    prior_0 = training_df[training_df[19] == 0].count()[0]/training_df.shape[0]\n",
    "\n",
    "    groups = training_df.groupby(training_df[19])\n",
    "    g1 = groups.get_group(1)\n",
    "    g0 = groups.get_group(0)\n",
    "    do_laplace = [0 for x in range(18)]\n",
    "\n",
    "    for i in range(18):\n",
    "        # Abhishek the line below is what we used yesterday, but since record is an int with the for each loop\n",
    "            # being used right now, this is an error.\n",
    "        rval = test_record[i]\n",
    "        \n",
    "        counts_given_1 = g1[g1[i] == rval].count()[0]\n",
    "        counts_given_0 = g0[g0[i] == rval].count()[0]\n",
    "        denom_1 = g1.shape[0]\n",
    "        denom_0 = g0.shape[0]\n",
    "        \n",
    "        # number of different values that attribute i can take. add this to denom for laplace\n",
    "        unique = len(training_df[i].unique())\n",
    "#         print(\"une value for column{col}: {num}\".format(col=i, num=unique))\n",
    "        \n",
    "        if counts_given_1 == 0 or counts_given_0 == 0:\n",
    "            counts_given_1 += 1\n",
    "            counts_given_0 += 1\n",
    "            denom_1 += unique\n",
    "            denom_0 += unique\n",
    "        prior_1 *= counts_given_1 / denom_1\n",
    "        prior_0 *= counts_given_0 / denom_0\n",
    "    # calculate ~probability of 0\n",
    "        # P(record[0]| class=0) * ... * P(record[18]| class=0) * P(class=0)\n",
    "#     for i in range(0,18):\n",
    "#         rval = test_record[i]\n",
    "#         counts = \n",
    "#         if counts == 0:\n",
    "#             do_laplace[i] = 1\n",
    "#         prior_0 *= counts / g0.shape[0]\n",
    "    # return greater of ~probabilities\n",
    "    \n",
    "    return 1 if prior_1 > prior_0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for fold 1\n",
      " [[71 36]\n",
      " [65 58]]\n",
      "Confusion Matrix for fold 2\n",
      " [[72 31]\n",
      " [59 68]]\n",
      "Confusion Matrix for fold 3\n",
      " [[70 38]\n",
      " [51 71]]\n",
      "Confusion Matrix for fold 4\n",
      " [[75 37]\n",
      " [61 57]]\n",
      "Confusion Matrix for fold 5\n",
      " [[77 32]\n",
      " [55 66]]\n",
      "Average Accuracy is:  0.5956521739130436\n",
      "\n",
      "Final Confusion Matrix\n",
      " [[365 174]\n",
      " [291 320]]\n",
      "Class 1 Precision: 0.647773 and Recall: 0.523732\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "\n",
    "df = get_data('messidor_features.txt')\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# TODO: trim off class label and hold separately\n",
    "\n",
    "sum_acc = 0\n",
    "\n",
    "#OUTPUT\n",
    "# 5-fold cross validation\n",
    "\n",
    "data_len = df.shape[0]\n",
    "interval = (int)(data_len / 5)\n",
    "\n",
    "# print(data_len)\n",
    "# print(interval)\n",
    "\n",
    "confusion_matrix = np.array([[0,0],[0,0]], np.int32)\n",
    "\n",
    "\n",
    "for i in range(0, data_len - interval, interval):\n",
    "    cur_matrix = np.array([[0,0],[0,0]], np.int32)\n",
    "    # partition data into train_set and test_set\n",
    "    train_set = df[0:i].append(df[i+interval:])\n",
    "    # test_set is deep copy\n",
    "    test_set = df[i:i+interval]\n",
    "    \n",
    "    #print(test_set.head())\n",
    "    \n",
    "    # get binned df (tuple)\n",
    "    binned_train_set, bins = bin_df(train_set)\n",
    "    \n",
    "    # binned_test_set is a shallow copy\n",
    "    binned_test_set = bin_test(test_set, bins)\n",
    "    \n",
    "    \n",
    "    # Abhishek maybe take a look at what this prints! It looks good overall but some values \n",
    "    # are coming in as 0s so I guess those were the outliers. Do we wanna just adjust\n",
    "    # all the buckets to have high maxs and low mins so they get put in either 1 or 3?\n",
    "#     print(binned_test_set.head())\n",
    "#    tester_record = pd.DataFrame([[1,   1,  2,  2,  2,  2,  1,  1,  2,  1,  1,  1,  1,  1,  1,  1,  2,  2,  1,  1],\n",
    "#                                 [3,   1,  2,  2,  2,  2,  1,  1,  2,  1,  1,  1,  1,  1,  1,  1,  2,  2,  1,  1]])\n",
    "#    print(classify(binned_train_set, tester_record.loc(0)))\n",
    "\n",
    "    # run testing set (!!!! This should be replaced)\n",
    "    \n",
    "    predicted_values = binned_test_set.apply(nb_classify, args=(binned_train_set,), axis=1)\n",
    "#     print(\"predicted_values\", predicted_values, type(predicted_values))\n",
    "    for idx, pred in predicted_values.items():\n",
    "#         print(\"binned_test_set\", binned_test_set.iloc(idx))\n",
    "#         print(\"Predicted %i for record %i\" %(idx, pred))\n",
    "#         print(\"record %i: \" % idx, binned_test_set.iloc[idx] )\n",
    "        actl = binned_test_set.loc[idx][19]\n",
    "#         print(\"Actual: \", actl)\n",
    "        cur_matrix[actl][pred] += 1\n",
    "    print(\"Confusion Matrix for fold %i\\n\" % (i//interval + 1), cur_matrix)\n",
    "    confusion_matrix += cur_matrix\n",
    "    \n",
    "#     for record in binned_test_set:\n",
    "#         print(\"guess:\", classify(binned_train_set, record), \"actual:\", record[19])\n",
    "        \n",
    "    # print and factor in new accuracy (store numbers for misplaced testing figures for confusion matrix)\n",
    "    # TN FP\n",
    "    # FN TP\n",
    "    \n",
    "    # TODO make \"accuracy\" = total of \n",
    "    accuracy = (cur_matrix[0][0] + cur_matrix[1][1])/len(predicted_values)\n",
    "    sum_acc += accuracy\n",
    "\n",
    "    \n",
    "print(\"Average Accuracy is: \", sum_acc/5)\n",
    "# output confusion matrix\n",
    "\n",
    "print(\"\\nFinal Confusion Matrix\\n\", confusion_matrix)\n",
    "\n",
    "predicted_ones = np.sum(confusion_matrix, dtype=np.int32, axis=0)[1]\n",
    "actually_ones  = np.sum(confusion_matrix, dtype=np.int32, axis=1)[1]\n",
    "correctly_predicted_ones = confusion_matrix[1][1]\n",
    "# print precision/recall of class 1\n",
    "precision = correctly_predicted_ones/predicted_ones\n",
    "recall = correctly_predicted_ones/actually_ones\n",
    "\n",
    "print(\"Class 1 Precision: %f and Recall: %f\" % (precision, recall))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2: K Nearest Neighbor (KNN) Classifier\n",
    "\n",
    "The KNN classifier consists of two stages:-\n",
    "- In the training stage, the classifier takes the training data and simply memorizes it\n",
    "- In the test stage, the classifier compares the test data with the training data and simply returns the maximum occuring label of the k nearest data points.\n",
    "\n",
    "The distance calculation method is central to the algorithm, typically Euclidean distance is used but other distance metrics like Manhattan distance can also be used. In this assignment you'll be implementing the classifier using the Euclidean distance metric. It is important to note that Euclidean distance is very sensitive to the scaling of different attributes hence, before you can build your classifier you have to normalize the values of each feature in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Normalize the dataset so that each feature value lies between `[0-1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "def normalize_df(df):\n",
    "    normalized_df=(df-df.min())/(df.max()-df.min())\n",
    "    return normalized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Build your KNN classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-9-c19df405dd4f>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-c19df405dd4f>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "def knn_classify(record, df, k):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Find the best value of k for this data. Try k ranging from 1 to 10. For each k value, use a 5-fold cross validation to evaluate the accuracy with that k. In each fold of CV, divide your data into a training set and a validation set. Print out the best value of k and the accuracy achieved with that value. Return the best value of k. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "def find_best_k(df):\n",
    "    data_len = df.shape[0]\n",
    "    interval = (int)(data_len / 5)\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    best_k = 1\n",
    "\n",
    "    for i in range(0, data_len - interval, interval):\n",
    "\n",
    "        # partition data into train_set and validation_set\n",
    "        train_set = df[0:i].append(df[i+interval:])\n",
    "        # validation_set is deep copy\n",
    "        validation_set = df[i:i+interval]\n",
    "        \n",
    "        for k in range(1, 10):\n",
    "            # call KNN classifier with k, train_set, and validation_set\n",
    "            # compute accuracy of results\n",
    "            accuracy = 0\n",
    "            if (accuracy >= best_accuracy):\n",
    "                best_accuracy = accuracy\n",
    "                best_k = k\n",
    "        \n",
    "        \n",
    "    print(\"Best k:\", best_k, \"Accuracy Achieved:\", best_accuracy)\n",
    "    return best_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Now measure the accuracy of your classifier using 5-fold cross validation. In each fold of this CV, divide your data into a training set and a test set. The training set should get sent through your code for Q4, resulting in a value of k to use. Using that k, calculate an accuracy on the test set. You will average the accuracy over all 5 folds to obtain the final accuracy measurement. Print the accuracy as well as the precision and recall for class label 1 (patients that have been diagnosed with the disease)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n",
    "df = get_data('messidor_features.txt')\n",
    "sum_acc = 0\n",
    "\n",
    "# 5-fold cross validation\n",
    "\n",
    "data_len = df.shape[0]\n",
    "interval = (int)(data_len / 5)\n",
    "\n",
    "# print(data_len)\n",
    "# print(interval)\n",
    "\n",
    "for i in range(0, data_len - interval, interval):\n",
    "\n",
    "    # partition data into train_set and test_set\n",
    "    train_set = df[0:i].append(df[i+interval:])\n",
    "    # test_set is deep copy\n",
    "    test_set = df[i:i+interval]\n",
    "    \n",
    "    norm_train_set = normalize_df(train_set)\n",
    "    norm_test_set = normalize_df(test_set)\n",
    "    \n",
    "    k = find_best_k(norm_train_set)\n",
    "    # call KNN classifier with k, norm_train_set, norm_test_set\n",
    "    # compute accuracy of result\n",
    "    accuracy = 0\n",
    "    sum_acc += accuracy\n",
    "\n",
    "    \n",
    "print(\"Average Accuracy is: \", sum_acc/5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
